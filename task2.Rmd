---
title: "Project Task 2"
author: "Aleksej Hoffaerber, Egor Zmaznev"
date: "4/1/2020"
output:
  pdf_document:
    df_print: paged
    toc: yes
    toc_depth: 3
---

\newpage
\listoffigures
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
# Libraries and theme----
library(fpp2)
library(ggplot2)
library(PerformanceAnalytics)
library(corrplot)
library(ggpubr)
library(seasonal)
library(splines)
library(dplyr)
library(magrittr)
library(readr)
library(skimr)
library(urca)
library(tsDyn)
library(ForecastComb)

theme_set(theme_minimal())
```


```{r loading data, echo=FALSE, message=FALSE, warning=FALSE}
# 1. Loading data ----
# Loading .csv file containing the important data points on "Net national disposable income" (di) and "Final consumption expenditure" (ce)
# for all the countries available. Filtering out unnecessary columns, and changing data format to a wide format
df <- read_csv("data.csv")

df %<>%
  filter(LOCATION == "AUS") %>% 
  select(SUBJECT, TIME, Value) %>%
  tidyr::spread(SUBJECT, Value) %>% # only calling single function to reduce conflicts between tidyr and dplyr
  rename(Date = TIME,
         di = B6NS1,
         ce = P3S1)

# creating time-series object
tse <- ts(df[,2:3], start = c(1959,3), end = c(2019,4), frequency = 4)

# binding date information from ts object to original df (easier than transforming the original format to a R date format)
df <- cbind(as.double(time(tse)), df) 

df %<>% select(-Date) %>% 
  rename(Date = "as.double(time(tse))")

# 2. Split into test and train set (10 years of prediction, 39 quarters) ----
ts.train <- window (tse,
               start = c(1959,3),
               end = c(2009,4),
               frequency = 4)

ts.test <- window (tse, 
              start = c(2010,1),
              frequency = 4)
```


## Time-series analysis and summaries
Having a look at the time series, we see a high amount of correlation between both series that needs to be analysed using the Augmented Dickey-Fuller (ADF) Test and corrected using Differencing and other transformations. We also observe that the seasonality and trend components are similar for both series. Distance between Consumption and Income increases with the later years the distance between the series, meaning that people in Australia earn more than they spend. Also, false regression issues can be discarded as a potential issue, as disposable available income is impacts the consumption of the population because consumption depends on earnings and debt available.

```{r Figure 1: Disposable Income Analysis, echo=FALSE, message=FALSE, warning=FALSE}
# Analysis of the disposable income ----
full.plot <- autoplot(tse[,1], color = "black", series = "asdasd") +
  autolayer(tse[,2], color = "darkblue", series = "dddd") +
  ggtitle("Net National Disposable Income and Final Consumption Expenditure", 
          subtitle = "Two time series show a high degree of correlation") +
  scale_x_continuous(name = "Years",
                     limits = c(1960, 2020),
                     breaks = seq(1960, 2020, by = 10)) +
  scale_y_continuous(name = "Disposable Income (in AUD millions)",
                     limits = c(0, 500000),
                     breaks = seq(0, 500000, by = 125000))

zoom.plot <- autoplot(tse[,1], color = "black", series = "asdasd") +
  autolayer(tse[,2], color = "darkblue", series = "dddd") +
  ggtitle("Net National Disposable Income and Final Consumption Expenditure", 
          subtitle = "High degree of correlation even clearer for trend and cycles after zoom-in") +
  scale_x_continuous(name = "Years",
                     limits = c(1990, 2020),
                     breaks = seq(1990, 2020, by = 10)) +
  scale_y_continuous(name = "Consumption Exp. (in AUD millions)",
                     breaks = seq(0, 500000, by = 125000))

ggarrange(full.plot, zoom.plot, nrow = 2) # Figure 1
```

Checking the residuals indicates that lots of autocorrelation remains in the residuals. In other words: valuable information that is currently not used to predict the data. This autocorrelation pattern is also typical for this type of economic data and time series. The significance of autocorrelation issue is very high, as indicated by the Ljung-Box test. Additionally, strong trend cycles and seasonality, visible in the ACF and residual plots indicate that both series are non-stationary. Based on the distribution of the residuals we also see, that stabilisation of variance and mean are particularly important in order to design a suitable forecasting model based on AR(I)MA.

### Disposable Income Analysis
To deal with the economic characteristic we apply a log-transformation to the data. But, this is not enough, as after the log-transformation not only the autocorrelation and heteroscedasticity issues remain, but also because the distibution of the residuals does not have a fitting Gauss distribution, as can be seen in _Figure 2_. To fix this, we apply differencing methods to stabilize the mean and maintain the interpretability of the model and its results. Before we start with ditfferencing, we apply the KPSS test in order to see, whether differencing is required:
```{r Figure 2: residuals analysis, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(tse[,1]) # Figure 2
tse[,1] %>% ur.kpss() %>% summary()
```
Based on this output, we see that the data is non-stationary, the null hypothesis indicating stationary data, can be rejected to all p-levels from 10 to 1 percent. To guess a fitting number of differences we must scrutinise the ACF plots: Based on the previous analyses, we know that seasons and trends play a role. A way to account for both issues and to keep the interpretability of the results is to apply first-order and seasonal differencing. In this way, we account for quarterly and seasonal (so yearly) difference in the series and can interpret the results as _quarterly changes_.

As can be seen in _Appendix 2_ and after applying log-transformation, seasonal, and first-order differencing, the ACF plots looks less impacted by issues, such as autocorrelation, heteroscedasticity, and in total: non-stationarity. The main and variance are now stabilised. But, there are still many spikes in the ACF and residuals, but those do not follow a specific pattern. Still, the Ljung-Box test indicates that a significant amount of autocorrelation remains in the residuals, which we can not get rid off based on our pre-processing toolset. Specific assumptions and calculations in the ARIMA setting will account for the last issues. Applying the KPSS test, we see that the data is now stationary. The difference in both test resultse is based on autocorrelation or serial correlation, which is a much stronger indication than simple stationarity.

```{r Ljung-Box test and KPSS summary, echo=FALSE, message=FALSE, warning=FALSE}
Box.test(diff(diff(log(tse[,1]),4),1), lag = 10, type = "Ljung-Box")
diff(diff(log(tse[,1]),4),1) %>% ur.kpss() %>% summary()
```

To be completely sure this transformation is correct we apply KPSS functions in order to determine lag values for the differencing. Unsuprisingly, KPSS indicates to use first-order and seasonal differencing.
```{r DI ndiffs, echo=FALSE, message=FALSE, warning=FALSE}
tse[,1] %>% log() %>% nsdiffs()
tse[,1] %>% log() %>% diff(lag = 4) %>% ndiffs()
```

### Consumption Expenditure Analysis
Exactly the same test and procedure will be applied to the Final Consumption Expenditure. Because both time series look very similar (see _Figure 1_), it can be inferred that a very similar transformation must be applied. This holds true, as the plots Appendix show.

```{r CE KPSS, echo=FALSE, message=FALSE, warning=FALSE}
tse[,2] %>% ur.kpss() %>% summary()
```

We incorporate both log, seasonal and first-order differencing transformed series into the data frame and ts objectwe incorporate both log, seasonal and first-order differencing transformed series into the data frame and ts object
```{r Log, diff and seasonal, echo=FALSE, message=FALSE, warning=FALSE}
Box.test(diff(diff(log(tse[,2]),4),1), lag = 10, type = "Ljung-Box")
diff(diff(log(tse[,2]),4),1) %>% ur.kpss() %>% summary()

tse[,2] %>% log() %>% nsdiffs()
tse[,2] %>% log() %>% diff(lag = 4) %>% ndiffs()

df$di.adj <- c(c(rep(NA,5), diff(diff(log(tse[,1]),4),1)))
df$ce.adj <- c(c(rep(NA,5), diff(diff(log(tse[,2]),4),1)))

tse <- ts(df[,2:5], start = c(1959,3), end = c(2019,4), frequency = 4)

# Split again into test and train set, with slightly adjusted start date to account for differencing reduction of the data ----
ts.train <- window(tse,
                   start = c(1960,4),
                   end = c(2009,4),
                   frequency = 4)

ts.test <- window(tse, 
                  start = c(2010,1),
                  end = c(2019,4),
                  frequency = 4)
```
We still observe stationarity in our series, but assume that the rest will be resolved employing the ARIMA machanics.
```{r Summary of the new parameters, echo=FALSE, message=FALSE, warning=FALSE}
summary(ur.df(ts.train[,3], type = "none"))
summary(ur.df(ts.train[,4], type = "none"))
```

##  Long-term relationship analysis
As already indicated above, just regressing both variables on each other might lead to spurious regressions, identifiable by a high Adj. R-sqaured and high residual autocorrelation (Hyndman & Athanasopoulos, 2018). This occurence impacts the reliance of our forecast in the long-term horizon. Based on the previously conducted KPSS test, we already know that non-stationarity and possible cointegration of our series are an issue. These unit root tests already indicated that in order to guarantuee that the characteristic equation lies within the unit circle (Hyndman & Athanasopoulos, 2018), we must take the differences in order to assure stationarity.

```{r TSLM with original and transformed data, echo=FALSE, message=FALSE, warning=FALSE}
fit.tse <- tslm(formula = ts.train[,1] ~ ts.train[,2]) 
summary(fit.tse)
summary(ur.df(residuals(fit.tse), type = "drift", lag = 1))

fit.tse.adj <- tslm(formula = ts.train[,3] ~ ts.train[,4]) 
summary(fit.tse.adj)
summary(ur.df(residuals(fit.tse.adj), type = "drift", lag = 1))
```
As we see, from both analyses, incorporating our transformed series for consumption into the regression does not resolve our non-stationarity issues in the regression setting. But this indicates, that both variables have a significant, cointegrated long-term relationship which can be used to design our forecasts. This also holds true if we apply the alternative values for tau2, being -3.43, -2.86, and -2.57.

### Identify ARIMA model for consumption expenditure
Firstly, based on the previously determined time series characteristics, we know that we must employ a first-order lag differences for our autoregression (AR) part and a first-order seasonal component for our quarterly data, meaining that d and D are equal to one in order to reflect the observations and KPSS test from before. Secondly, we test for an optimal value for p and q, based on ACF and PACF test. Because we already know that we have high autocorrelation in our data. A spike in the ACF and PACF in Figure XYZ indicates that we have significant spikes for lag 1 in both charts (Hyndman & Athanasopoulos, 2018), meaning that we should set p = q = 1.

```{r Figure 3: log-diff, echo=FALSE, message=FALSE, warning=FALSE}
log(ts.train[,2]) %>% 
  diff(lag = 4) %>% 
  diff() %>% 
  ggtsdisplay() # Figure 3
```

Based on the resulting assumptions, we conlude that an ARIMA(0,1,3)(0,1,2)[4] might be suitable, because of the significant spikes seen in the PACF for lag 3 and 6, determining q = 3 and Q = 3.

The subsequent check shows some autocorrelation left, visible in the ACF plot at spike 6. Because we already adjusted the MA(q) part, this must be a detail to be adjusted in the AR(p) part. We therefore set p = 1 and 2 and compare their AICc, autocorrelation, stationaroty and white noise residuals.

```{r Fit of the first models, echo=FALSE, message=FALSE, warning=FALSE}
fit.1 <- Arima(ts.train[,2], order = c(0,1,3), seasonal = c(0,1,2), lambda = BoxCox.lambda(ts.train[,2]))
fit.2 <- Arima(ts.train[,2], order = c(1,1,3), seasonal = c(0,1,2), lambda = BoxCox.lambda(ts.train[,2]))
```


```{r Figure 4: ARIMA (2,1,3)(0,1,2)[4], echo=FALSE, message=FALSE, warning=FALSE}
(fit.3 <- Arima(ts.train[,2], order = c(2,1,3), seasonal = c(0,1,2), lambda = BoxCox.lambda(ts.train[,2])))
ce.acf.3 <- ggAcf(fit.3$residuals) + ylab("") + ggtitle("ACF for ARIMA(2,1,3)(0,1,2)")
ce.pacf.3 <- ggPacf(fit.3$residuals) + ylab("") + ggtitle("PACF for ARIMA(2,1,3)(0,1,2)")
ggarrange(ce.acf.3, ce.pacf.3, ncol = 2) # Figure 4

```

```{r Figure 5: residual analysis, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(fit.3) # Figure 5
```

```{r Figure 6: characteristic roots, echo=FALSE, message=FALSE, warning=FALSE}
autoplot(fit.3) # Figure 6
```

Comparing our models for ARIMA(1,1,3)(0,1,2)[4] and ARIMA(2,1,3)(0,1,2)[4], we see that the latter has better ACF and PACF spike conditions. Also, the Ljung-Box test is supporting this assumptions as the autocorrelation in the ARIMA(1,1,3)(0,1,2)[4] and ARIMA(0,1,3)(0,1,2)[4] is still highly significant as opposed by the latter model. The resulting series is now a white-noise series. The distribution of the residuals also fits the assumed distribution pattern. Looking at the coefficients, we observe that ar1, ma1, and sma2 are not statistically significant, and ma3 being almost not statistically significant. Because ar1 should not be altered because of high significance of ar2, we try new model variants:

```{r Modified fit 4, echo=FALSE, message=FALSE, warning=FALSE}
(fit.4 <- Arima(ts.train[,2], order = c(2,1,2), seasonal = c(0,1,1), lambda = BoxCox.lambda(ts.train[,2])))
ce.acf.4 <- ggAcf(fit.4$residuals) + ylab("") + ggtitle("ACF for ARIMA(2,1,2)(0,1,1)") + theme_minimal()
ce.pacf.4 <- ggPacf(fit.4$residuals) + ylab("") + ggtitle("PACF for ARIMA(2,1,2)(0,1,1)") + theme_minimal()
```

Because in our `fit4` model we still have some issues with autocorrelation we set P = 1 because of the trend that can still be observed in the residuals. We end up with an ARIMA model that also surpasses all previous models in terms of coefficient significance, and AICc, AIC, and BIC. Also the Ljung-Box p-value is maximized, as seen in _Figures 7 to 9_:

```{r Figure 7: model, ACF and PACF, echo=FALSE, message=FALSE, warning=FALSE}
(fit.5 <- Arima(ts.train[,2], order = c(2,1,2), seasonal = c(1,1,1), lambda = BoxCox.lambda(ts.train[,2])))
ce.acf.5 <- ggAcf(fit.5$residuals) + ylab("") + ggtitle("ACF for ARIMA(2,1,2)(1,1,1)") + theme_minimal()
ce.pacf.5 <- ggPacf(fit.5$residuals) + ylab("") + ggtitle("PACF for ARIMA(2,1,2)(1,1,1)") + theme_minimal()
ggarrange(ce.acf.5, ce.pacf.5, ncol = 2) # Figure 7
```

```{r Figure 8: residual analysis, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(fit.5) # Figure 8
```

```{r Figure 9: characterisitc roots, echo=FALSE, message=FALSE, warning=FALSE}
autoplot(fit.5) # Figure 9
```
For detailed figures and graphs showing the other scenarios, please see the Appendix.

Because KPSS can only be used to determine d and D, we need to employ Information Criteria, such as AICc, to pick the correct p,q,P,Q values. This is already incorporated in the automated ARIMA model selection that calculates different ARIMA models and picks the best models based on those Informaiton Criteria. In fact, the same ARIMA(2,1,2)(1,1,1)[4] is picked, based on the unit root space optimization to guarantee stationarity. Also, the obtained ARIMA coefficients remain highly statistically singificant, aside of ma1.
```{r AutoARIMA, message=FALSE, warning=FALSE, include=FALSE}
(fit.arima <- auto.arima(ts.train[,2], lambda = BoxCox.lambda(ts.train[,2])))
summary(fit.arima) # no print
checkresiduals(fit.arima) # no print
```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```
## Appendix
```{r Appendix 1 Ljung-Box test, echo=FALSE, message=FALSE, warning=FALSE}
Box.test(tse[,1], lag = 10, type = "Ljung-Box") # Appendix 1
```

```{r Appendix 2: log residual analysis , echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(log(tse[,1])) # Appendix 2
```

```{r Appendix 3: second order diff-log analysis: , echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(diff(diff(log(tse[,1]),4),1)) # Appendix 3
```

```{r Appendix 4: residual analysis CE: , echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(tse[,2]) # Appendix 4
Box.test(tse[,2], lag = 10, type = "Ljung-Box")
```

```{r Appendix 5: second/roder diff-log CE: , echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(diff(diff(log(tse[,2]),4),1)) # Appendix 5
```

```{r Appendix 6: first ARIMA model, echo=FALSE, message=FALSE, warning=FALSE}
(fit.1 <- Arima(ts.train[,2], order = c(0,1,3), seasonal = c(0,1,2), lambda = BoxCox.lambda(ts.train[,2])))
ce.acf.1 <- ggAcf(fit.1$residuals) + ylab("") + ggtitle("ACF for ARIMA(0,1,3)(0,1,2)")
ce.pacf.1 <- ggPacf(fit.1$residuals) + ylab("") + ggtitle("PACF for ARIMA(1,1,1)(0,1,2)")
ggarrange(ce.acf.1, ce.pacf.1, ncol = 2) # Appendix 5
```

```{r Appendix 7: second ARIMA model, echo=FALSE, message=FALSE, warning=FALSE}
(fit.2 <- Arima(ts.train[,2], order = c(1,1,3), seasonal = c(0,1,2), lambda = BoxCox.lambda(ts.train[,2])))
ce.acf.2 <- ggAcf(fit.2$residuals) + ylab("") + ggtitle("ACF for ARIMA(1,1,3)(0,1,2)")
ce.pacf.2 <- ggPacf(fit.2$residuals) + ylab("") + ggtitle("PACF for ARIMA(1,1,3)(0,1,2)")
ggarrange(ce.acf.2, ce.pacf.2, ncol = 2) # Appendix 6
```

```{r Appendix 8: residual analysis, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(fit.1)$p.value # Appendix 7
```

```{r Appendix 9: residual analysis, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(fit.2) # Appendix 8
```

```{r Appendix 10: ACF and PACF, echo=FALSE, message=FALSE, warning=FALSE}
ggarrange(ce.acf.4, ce.pacf.4, ncol = 2) # Appendix 9
```

```{r Appendix 11> residuals and tests comparison, echo=FALSE, message=FALSE, warning=FALSE}
(arima.comp.1 <- data.frame(model=c("ARIMA(1,1,1)(0,1,2)", "ARIMA(1,1,3)(0,1,2)", "ARIMA(2,1,3)(0,1,2)", "ARIMA(2,1,2)(0,1,1)", "ARIMA(2,1,2)(1,1,1)"),
                            LB.p.value = c(checkresiduals(fit.1)$p.value, checkresiduals(fit.2)$p.value, checkresiduals(fit.3)$p.value, checkresiduals(fit.4)$p.value, checkresiduals(fit.5)$p.value),
                            aicc = c(fit.1$aicc, fit.2$aicc, fit.3$aicc, fit.4$aicc, fit.5$aicc),
                            bic = c(fit.1$bic, fit.2$bic, fit.3$bic, fit.4$bic, fit.5$bic)) %>% 
    mutate_if(is.numeric, round, digit = 3) %>% 
    arrange(desc(LB.p.value), aicc)) # ordering by p.value and AICc

```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

```{r , echo=FALSE, message=FALSE, warning=FALSE}


```

## References
